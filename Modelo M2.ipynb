{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Todos.csv', sep=';')\n",
    "df = df.drop(['Operacoes_medicoes', 'Temperatura_ensaio','Operacoes_anual', 'Nivel de atrito'], axis=1)\n",
    "X = df.loc[:, ['Distancia_medicao', 'Lado', 'Remocao_anterior', 'Idade_pavimento_meses', 'Umidade_AISWEB', 'Operacoes_remocoes']]\n",
    "Y = df['Atrito']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os conjuntos de dados entre 90% para Treinamento e 10% para Teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "print('Shape X_train:', X_train.shape) # Imprime a matriz dos dados de entrada de treinamento\n",
    "print('Shape X_test:', X_test.shape) # Imprime a matriz dos dados de saída de treinamento\n",
    "print('Shape y_train:', y_train.shape) # Imprime a matriz dos dados de entrada de teste\n",
    "print('Shape y_test:', y_test.shape)  # Imprime a matriz dos dados de saída de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATENÇÃO: Na célula abaixo apenas as entradas são normalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processando os dados de entrada utilizando o pré-processamento Standard Scaler\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(X_train) # Ajustando os dados para a média e desvio padrão do conjunto de teste\n",
    "X_train = scaler_x.transform(X_train) # Aplicando a transformação no conjunto de treinamento\n",
    "X_test = scaler_x.transform(X_test) # O conjunto de teste é pré-processado de acordo com o conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(\n",
    "            hidden_layer_sizes=(94, 73),\n",
    "            activation='relu',\n",
    "            solver='lbfgs',\n",
    "            alpha=0.1,\n",
    "            random_state=3,\n",
    "            max_iter=600)\n",
    "        \n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_treinamento = reg.score(X_train, y_train)\n",
    "score_teste = reg.score(X_test, y_test)\n",
    "print('R² treinamento: %.2f' %(score_treinamento*100), ' R² teste: %.2f' %(score_teste*100))\n",
    "\n",
    "MAE_treinamento = mean_absolute_error(y_train, reg.predict(X_train))\n",
    "MAE_teste = mean_absolute_error(y_test, reg.predict(X_test))\n",
    "print('MAE treinamento: %.4f' %MAE_treinamento, ' MAE teste: %.4f' %MAE_teste)\n",
    "\n",
    "MSE_treinamento = mean_squared_error(y_train, reg.predict(X_train))\n",
    "MSE_teste = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print('MSE treinamento: %.4f' %MSE_treinamento, ' MSE teste: %.4f' %MSE_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algumas informações acerca da Rede Neural Artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = reg.loss_\n",
    "coeficientes = reg.coefs_\n",
    "interceptor = reg.intercepts_\n",
    "n_iter = reg.n_iter_\n",
    "n_iter_no_change = reg.n_iter_no_change\n",
    "n_layer = reg.n_layers_\n",
    "output = reg.n_outputs_\n",
    "output_function = reg.out_activation_\n",
    "print('Loss: %.5f\\n' %loss, \n",
    "      'Iterações: %.i\\n' %n_iter, \n",
    "      'Iterações sem mudança: %.i\\n' %n_iter_no_change, \n",
    "      'Nº de camadas: %i\\n' %n_layer,\n",
    "      'Saída: %.i\\n' %output,\n",
    "      'Função de ativação de saída: %s' %output_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeficientes_entrada_1_cam_oculta = coeficientes[0]\n",
    "coeficientes_1_cam_oculta_2_cam_oculta = coeficientes[1]\n",
    "coeficientes_2_cam_oculta_saida = coeficientes[2]\n",
    "interceptor_entrada_1_cam_oculta = interceptor[0]\n",
    "interceptor_1_cam_oculta_2_cam_oculta = interceptor[1]\n",
    "interceptor_2_cam_oculta_saida = interceptor[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvandos os pesos e os bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('coeficientes_entrada_1_cam_oculta.txt',\n",
    "           coeficientes_entrada_1_cam_oculta,\n",
    "           fmt='%.4f',\n",
    "           comments='# Coeficientes da camada de entrada para a 1 camada oculta')\n",
    "\n",
    "np.savetxt('coeficientes_1_cam_oculta_2_cam_oculta.txt',\n",
    "           coeficientes_1_cam_oculta_2_cam_oculta,\n",
    "           fmt='%.4f',\n",
    "           comments='# Coeficientes da 1 camada oculta para 2 camada oculta')\n",
    "\n",
    "np.savetxt('coeficientes_2_cam_oculta_saida.txt',\n",
    "           coeficientes_2_cam_oculta_saida,\n",
    "           fmt='%.4f',\n",
    "           comments='# Coeficientes da 2 camada oculta para a camada de saida')\n",
    "\n",
    "np.savetxt('interceptor_entrada_1_cam_oculta.txt',\n",
    "           interceptor_entrada_1_cam_oculta,\n",
    "           fmt='%.4f',\n",
    "           comments='# Interceptor da camada de entrada para a 1 camada oculta')\n",
    "\n",
    "np.savetxt('interceptor_1_cam_oculta_2_cam_oculta.txt',\n",
    "           interceptor_1_cam_oculta_2_cam_oculta,\n",
    "           fmt='%.4f',\n",
    "           comments='# Interceptor da 1 camada oculta para 2 camada oculta')\n",
    "\n",
    "np.savetxt('interceptor_2_cam_oculta_saida.txt',\n",
    "           interceptor_2_cam_oculta_saida,\n",
    "           fmt='%.4f',\n",
    "           comments='# Interceptor da 2 camada oculta para a camada de saida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')# default volta ao padrão\n",
    "y_train_predicted = pd.Series(reg.predict(X_train))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_train, y_train_predicted, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Coeficiente de atrito observado', fontsize=16)\n",
    "plt.ylabel('Coeficiente de atrito estimado', fontsize=16)\n",
    "plt.title('Treinamento', fontsize=17)\n",
    "\n",
    "b, m = polyfit(y_train, y_train_predicted, 1)\n",
    "plt.plot(y_train, b + m * y_train, '-', color='k', alpha=0.5)\n",
    "plt.text(0.5, 0.95, \"y = %.3fx + %.3f\"%(m, b), fontsize=14)\n",
    "plt.text(0.525, 0.92, 'R2 %.2f%%' %(reg.score(X_train, y_train)*100), fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafico_dispersao_treinamento_M2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = pd.Series(reg.predict(X_test))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, reg.predict(X_test), alpha=0.5)\n",
    "plt.xlabel('Coeficiente de atrito observado', fontsize=16)\n",
    "plt.ylabel('Coeficiente de atrito estimado', fontsize=16)\n",
    "plt.title('Teste', fontsize=16)\n",
    "\n",
    "b, m = polyfit(y_test, y_test_predicted, 1)\n",
    "plt.plot(y_test, b + m * y_test, '-', color='k', alpha=0.5)\n",
    "plt.text(0.55, 0.9, \"y = %.3fx + %.3f\"%(m, b), fontsize=14)\n",
    "plt.text(0.575, 0.875, 'R2 %.2f%%' %(reg.score(X_test, y_test)*100), fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.savefig('Grafico_dispersao_teste_M2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_treinamento = pd.Series(y_train - reg.predict(X_train))\n",
    "erro_teste = pd.Series(y_test - reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_treinamento.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros_neg_treinamento = (len(erro_treinamento[erro_treinamento < 0]) / len(erro_treinamento)) * 100\n",
    "erros_pos_treinamento = (len(erro_treinamento[erro_treinamento > 0]) / len(erro_treinamento)) * 100\n",
    "print('Erros abaixos de 0: %.2f%%\\n' %erros_neg_treinamento, 'Erros acima de 0: %.2f%%\\n' %erros_pos_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "bins = np.arange(-0.25, 0.20, 0.025)\n",
    "plt.hist(erro_treinamento, edgecolor='black', bins=bins) # comprimento da barra 0.03322844\n",
    "plt.title('Histograma dos erros do treinamento', fontsize=18)\n",
    "plt.ylabel('Frequência', fontsize=17)\n",
    "plt.xlabel('Erro (observado - previsto)', fontsize=16)\n",
    "plt.axvline(erro_treinamento.mean(), color='#f23843', label='Erro médio')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.savefig('Histograma_erro_treinamento_M2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_teste.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros_neg_teste = (len(erro_teste[erro_teste < 0]) / len(erro_teste)) * 100\n",
    "erros_pos_teste = (len(erro_teste[erro_teste > 0]) / len(erro_teste)) * 100\n",
    "print('Erros abaixos de 0: %.2f%%\\n' %erros_neg_teste, 'Erros acima de 0: %.2f%%\\n' %erros_pos_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "bins = np.arange(-0.15, 0.20, 0.025)\n",
    "plt.hist(erro_teste, edgecolor='black', bins=bins) # comprimento da barra 0.02417327‬\n",
    "plt.title('Histograma dos erros do teste', fontsize=16)\n",
    "plt.ylabel('Frequência', fontsize=14)\n",
    "plt.xlabel('Erro (observado - previsto)', fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.axvline(erro_teste.mean(), color='#f23843', label='Erro médio')\n",
    "plt.legend()\n",
    "plt.savefig('Histograma_erro_teste_M2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variação das médias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_menos_1_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()-X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()-X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()-X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_menos_1_std = x_menos_1_std.T\n",
    "x_menos_1_std.loc[(x_menos_1_std.Operacoes_remocoes < 0),'Operacoes_remocoes'] = 0\n",
    "\n",
    "\n",
    "x_menos_2_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()-2*X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()-2*X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()-2*X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_menos_2_std = x_menos_2_std.T\n",
    "x_menos_2_std.loc[(x_menos_2_std.Operacoes_remocoes < 0),'Operacoes_remocoes'] = 0\n",
    "\n",
    "\n",
    "x_menos_3_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()-3*X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()-3*X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()-3*X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_menos_3_std = x_menos_3_std.T\n",
    "x_menos_3_std.loc[(x_menos_3_std.Operacoes_remocoes < 0),'Operacoes_remocoes'] = 0\n",
    "x_menos_3_std.loc[(x_menos_3_std.Idade_pavimento_meses < 0),'Idade_pavimento_meses'] = 0\n",
    "\n",
    "x_mais_1_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()+X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()+X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()+X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_mais_1_std = x_mais_1_std.T\n",
    "x_mais_1_std['Operacoes_remocoes'] = x_mais_1_std['Operacoes_remocoes'].map(format)\n",
    "\n",
    "x_mais_2_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()+2*X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()+2*X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()+2*X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_mais_2_std = x_mais_2_std.T\n",
    "x_mais_2_std['Operacoes_remocoes'] = x_mais_2_std['Operacoes_remocoes'].map(format)\n",
    "\n",
    "x_mais_3_std = pd.DataFrame([np.concatenate((4*[np.arange(200, 2600, 100)])), \n",
    "                  np.concatenate((np.zeros(24), np.ones(24), np.zeros(24), np.ones(24))), \n",
    "                  np.concatenate((np.zeros(48), np.ones(8), np.zeros(16), np.ones(8), np.zeros(16))), \n",
    "                  96*[X['Idade_pavimento_meses'].mean()+3*X['Idade_pavimento_meses'].std()], \n",
    "                  96*[X['Umidade_AISWEB'].mean()+3*X['Umidade_AISWEB'].std()], \n",
    "                  96*[X['Operacoes_remocoes'].mean()+3*X['Operacoes_remocoes'].std()]],\n",
    "                index=X.columns)\n",
    "x_mais_3_std = x_mais_3_std.T\n",
    "x_mais_3_std['Operacoes_remocoes'] = x_mais_3_std['Operacoes_remocoes'].map(format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_testes = pd.concat((x_menos_1_std, x_menos_2_std, x_menos_3_std, x_mais_1_std, x_mais_2_std, x_mais_3_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando com variações da média de algumas variáveis\n",
    "todos_testes = scaler_x.transform(todos_testes)\n",
    "y_previsto = pd.Series(reg.predict(todos_testes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_previsto.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0.5, 1.05, 0.05)\n",
    "plt.hist(y_previsto, ec='black', bins=bins)\n",
    "plt.xlabel('Coeficiente de atrito', fontsize=16)\n",
    "plt.ylabel('Frequência', fontsize=16)\n",
    "plt.title('Histograma de coeficiente de atrito', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.axvline(y_previsto.mean(), label='coeficiente de atrito médio', color='red')\n",
    "#plt.legend(fontsize=12)\n",
    "plt.savefig('Histograma_de_coeficiente_de_atrito.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando a Rede Neural Artificial (RNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**É importante que os dados de entradas estejam nessa ordem, separados por vírgula(,):**<br> \n",
    "dados_de_entrada = Distancia_medicao, Lado, Remocao_anterior, Idade_pavimento_meses, Umidade_AISWEB, Operacoes_remocoes<br>\n",
    "\n",
    "**Explicação sobre as variáveis de entrada:**<br>\n",
    "1) Distancia_medicao: As distâncias de medição devem ser preenchidas em trechos de 100m, inciando a partir de 200m até 2500m (Ex.: 200, 300, 400...);<br>\n",
    "\n",
    "2) Lado: Os lados de medição devem ser preenchidos com 0 para o lado esquerdo e 1 para o lado direito;<br>\n",
    "\n",
    "3) Remocao_anterior: A remoção_anterior devem ser preenchida com 0, caso não tenha sido realizada remoção do acúmulo de borracha na pista de pouso e decolagem, e 1, caso tenha ocorrido. Deve-se considerar a realização de remoção somente entre os trechos de 200m a 900m, nos demais trechos inserir 0;<br>\n",
    "\n",
    "4) Idade_pavimento_meses: A idade do pavimento leva em consideração a quantidade de meses desde que a pista de pouso e decolagem foi recapeada até o presente momento;<br>\n",
    "\n",
    "5) Umidade_AISWEB: Umidade relativa do ar obtida do site http://clima.icea.gov.br/clima/index.php;\n",
    "\n",
    "6) Operacoes_remocoes: Nº de operações entre remoções, considerando pousos e decolagens, obtidos no site da ANAC;<br>\n",
    "\n",
    "As saídas da RNA são estimativas de coeficiente de atrito medidos com GripTester a 65 km/h a 3m do eixo central da pista de pouso e decolagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de aplicação\n",
    "dados_entrada = pd.DataFrame([[200, 0, 0, 80, 65, 10000],[200, 1, 0, 80, 65, 10000]], columns=X.columns) # Os dados são salvos em uma variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dados_entrada # Os dados exibidos abaixo vão estimar o coeficiente de atrito no trecho de 200m, nos lados esquerdo e\n",
    "              # direito, sem realização de remoção de acúmulo de borracha, considerando 80 meses de idade, umidade \n",
    "              # relativa do ar em 65% e 10.000 operações de pousos e decolagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz-se o pré-processamento os dados com o Standard Scaler\n",
    "dados_entrada_normalizados = scaler_x.transform(dados_entrada) # Salva-se os dados_entrada normalizados em uma nova variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeficiente_atrito_previsto = reg.predict(dados_entrada_normalizados) # Estimando os valores de coeficiente de atrito\n",
    "print('Coeficiente de atrito previsto do lado esquerdo: %.2f\\n' %coeficiente_atrito_previsto[0], \n",
    "      'Coeficiente de atrito previsto do lado direito: %.2f\\n' %coeficiente_atrito_previsto[1],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
